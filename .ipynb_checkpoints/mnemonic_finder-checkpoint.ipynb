{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of consonants and combinations of consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consonants:  20\n",
      "['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z']\n",
      "\n",
      "Consonant combinations:  190\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import string, re, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "alphabet_lowercase = list(string.ascii_lowercase)\n",
    "vowels = [\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"]\n",
    "consonants = [letter for letter in alphabet_lowercase if letter not in vowels ]\n",
    "\n",
    "consonant_pairs = itertools.combinations(\"\".join(consonants), 2)\n",
    "consonant_pairs = [letter for letter in consonant_pairs]\n",
    "consonant_pairs = [pair[0]+pair[1] for pair in consonant_pairs]\n",
    "\n",
    "print(\"Consonants:  \" + str(len(consonants)) +\"\\n\" + str(consonants))\n",
    "print(\"\\nConsonant combinations:  \" + str(len(consonant_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract h, y, b, c, w, x, z = \"dfgjklmnpqrstv\"\n",
    "\n",
    "Make a list of combinations of \"dfgjklmnpqrstv\"\n",
    "\n",
    "The reason for removing b and c is because I want them to be in there own group, since they are such high frequency letters.  The reason for removing wxz is because they are lower frequency letters, so they can be in a group of 3.  I am omitting h because it will be ignored in the system along with the vowels on account, for the purpose of reducing the number of letters I need to deal with, as well as for historical consistency with the major system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combos_14 = itertools.combinations(\"dfgjklmnpqrstv\", 2)\n",
    "combos_14 = list(combos_14)\n",
    "combos_14 = [letter[0]+letter[1] for letter in combos_14]\n",
    "print(\"Combinations of consonants without h, y, b, c, w, x, z:  \" + str(len(combos_14)))\n",
    "print(combos_14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter pairs from this list that produce low frequency matches according to the regex:\n",
    "\n",
    "`[aeiouyh]*[pair]+[aeiouyh]*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns=[\"matches\"]\n",
    "df = pd.DataFrame(index=combos_14, columns=columns)\n",
    "\n",
    "for i in combos_14:\n",
    "    re_list = [\"[aeiouhy]*\"]\n",
    "    matches = []\n",
    "    \n",
    "    p = \"[\"+i+\"]\"+\"+\"\n",
    "    re_list.append(p)\n",
    "    re_list.append(\"[aeiouhy]*\")\n",
    "    p = re.compile(\"\".join(re_list))\n",
    "    \n",
    "    for synset in list(wn.all_synsets('n')):\n",
    "        word = synset.name().split(\".\")[0]\n",
    "        if p.match(word):\n",
    "            matches.append(word)\n",
    "            \n",
    "    df.loc[i,[\"matches\"]] = len(matches)\n",
    "\n",
    "df.to_csv(\"match_freqs_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_data = \"match_freqs_10\"\n",
    "df = pd.read_csv(matches_data+\".csv\", index_col=0)\n",
    "df = df.sort_values(\"matches\", ascending=False)\n",
    "# df.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group = []\n",
    "for i in df.index:\n",
    "    if i[0] not in \"\".join(group) and i[1] not in \"\".join(group):\n",
    "        group.append(i)\n",
    "\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_char_dic = {0: \"sz\",\n",
    "                1: \"td\",\n",
    "                2: \"nm\",\n",
    "                3: \"ck\",\n",
    "                4: \"rl\",\n",
    "                5: \"fv\",\n",
    "                6: \"hj\",\n",
    "                7: \"gq\",\n",
    "                8: \"wx\",\n",
    "                9: \"pb\"}\n",
    "\n",
    "replacements = ((\"sz\", \"0\"),\n",
    "                (\"td\", \"1\"),\n",
    "                (\"nm\", \"2\"),\n",
    "                (\"ck\", \"3\"),\n",
    "                (\"rl\", \"4\"),\n",
    "                (\"fv\", \"5\"),\n",
    "                (\"hj\", \"6\"),\n",
    "                (\"gq\", \"7\"),\n",
    "                (\"wx\", \"8\"),\n",
    "                (\"pb\", \"9\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stein'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"ssteinn\"\n",
    "\n",
    "\n",
    "\n",
    "word\n",
    "\n",
    "# for letter in word:\n",
    "#     if word[word.index(letter)+1] == letter:\n",
    "#         del letter\n",
    "# print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = iter(word)\n",
    "next(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove vowels and consecutive duplicate letters\n",
    "\n",
    "Convert word to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stn\n",
      "012\n"
     ]
    }
   ],
   "source": [
    "word = \"steinn\"\n",
    "word = word.lower()\n",
    "word = re.sub(\"[aeiou\\W]\", \"\", word)\n",
    "word = re.sub(r'([a-z])\\1+', r'\\1', word)\n",
    "print(word)\n",
    "\n",
    "number = []\n",
    "for letter in word:\n",
    "    for lset, num in replacements:\n",
    "        if letter in lset:\n",
    "            number.append(num)\n",
    "\n",
    "print(\"\".join(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-cd9d0a205bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrew/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mall_synsets\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m   1517\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                             \u001b[0;31m# Otherwise, parse the line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m                             \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_pos_and_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m                             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrew/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_synset_from_pos_and_line\u001b[0;34m(self, pos, data_file_line)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0mlex_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;31m# If the lemma has a syntactic marker, extract it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(.*?)(\\(.*\\))?$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0mlemma_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_mark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 \u001b[0;31m# create the lemma object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrew/anaconda3/lib/python3.5/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[1;32m    162\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns=[\"matches\"]\n",
    "nums_from_words = []\n",
    "df = pd.DataFrame(index=range(0,10000), columns=columns)\n",
    "\n",
    "for synset in list(wn.all_synsets('n')):\n",
    "    word = synset.name().split(\".\")[0]\n",
    "    word = word.lower()\n",
    "    word = re.sub(\"[aeiou\\W]\", \"\", word)\n",
    "    word = re.sub(r'([a-z])\\1+', r'\\1', word)\n",
    "    \n",
    "    number = []\n",
    "    for letter in word:\n",
    "        for lset, num in replacements:\n",
    "            if letter in lset:\n",
    "                number.append(num)\n",
    "    \n",
    "    if len(number) < 5:\n",
    "        nums_from_words.append(\"\".join(number))\n",
    "        \n",
    "print(len(nums_from_words))\n",
    "nums_from_words = Counter(nums_from_words)\n",
    "print(nums_from_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a dictionary where the keys are all 20 consonants b-z (not including y).  The values for each key are a list of the 19 unique combinations with every other consonant, i.e. bc, bd, etc.  Now I want to get all possible combinations of putting thees consonant pairs into groups of 10, where each conosonant only appears once.  How can I achieve this by looping through my dictionary?  The loop should append the pair to a list if the letters in that pair are not yet in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combos_14_pairs = itertools.combinations(combos_14, 2)\n",
    "combos_14_pairs = list(combos_14_pairs)\n",
    "\n",
    "eliminate_combos = []\n",
    "for i in combos_14_pairs:\n",
    "    combo_string = i[0]+i[1]\n",
    "    letters_present = list(set(combo_string))\n",
    "    for l in letters_present:\n",
    "        if combo_string.count(l) > 1:\n",
    "            filtered_combos.append(combo_string)\n",
    "\n",
    "print(eliminate_combos[0:10])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"dfdg\".count(\"dg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the combinations in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for consonant in consonants:\n",
    "    pair_list = []\n",
    "    for pair in consonant_pairs:  \n",
    "        if consonant in pair:\n",
    "            pair_list.append(pair)\n",
    "    groups[consonant] = pair_list\n",
    "\n",
    "for k,v in sorted(groups.items()):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of lists\n",
    "The list needs to contain exactly 10 items\n",
    "Each item is a pair of consonants, i.e. bc, dz, etc.\n",
    "Each consonant can only appear in the list once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs_list = []\n",
    "for k,v in sorted(groups.items()):\n",
    "    pairs_list.append(v)\n",
    "    \n",
    "print(pairs_list)\n",
    "\n",
    "# ten_pairs = []\n",
    "# for i in pairs_list:\n",
    "#     if i[0] not in ten_pairs:\n",
    "#         ten_pairs.append(i[0])\n",
    "#     else\n",
    "    \n",
    "#     ten_pairs = []\n",
    "#     for i in v:\n",
    "#         if i[0] and i[1] not in ten_pairs:\n",
    "#             ten_pairs.append(i)\n",
    "#     print(ten_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set without vowels from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_vowel = re.compile(\"[aeiouwhy]\")\n",
    "letters = \"cell\"\n",
    "letters_list = [i for i in letters if not p_vowel.match(i)]\n",
    "\n",
    "letters_list = (list(set(letters_list)))\n",
    "print(letters_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a regex from the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_list = [\"[aeiouwhy]*\"]\n",
    "\n",
    "for i in letters_list:\n",
    "    re_list.append(i + \"{1,2}\" + \".*\")\n",
    "\n",
    "re_list.append(\"[aeiouwhy]*\")\n",
    "    \n",
    "# final_re = re.compile(\"([aeiou])*b\")\n",
    "\n",
    "final_re = \"\".join(re_list)\n",
    "print(final_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(final_re)\n",
    "\n",
    "matches= []\n",
    "for synset in list(wn.all_synsets('n')):\n",
    "    word = synset.name().split(\".\")[0]\n",
    "    if p.match(word):\n",
    "        matches.append(word)\n",
    "\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in matches[:10]:\n",
    "#     if len(i) <8:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# letter pairs\n",
    "p = re.compile(\"[aeiouy]*[cs]+[aeiouy]+[vl]+\")\n",
    "matches= []\n",
    "for synset in list(wn.all_synsets('n')):\n",
    "    word = synset.name().split(\".\")[0]\n",
    "    if p.match(word):\n",
    "        matches.append(word)\n",
    "\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in matches[:]:\n",
    "    if len(i) <5:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
